\section{Implementation of the Proposed Algorithm}
\label{sec:implementation}

Having introduced the algorithms of building a second order recursive filter by seperating into 
particular and homogeneous functions,
in this section we systematically discuss and compare with 
different combinations of functions of composing a second order or higher order recursive filter from implementation 
perspective, i.e., the cost in the number of instructions. 

Three different options of composing the second order filter are given with the block diagram as follows:

1. The option 1 (benchmark): The block filtering algorithm, which only accepts vector of samples. In particular, since the block filtering
deals with non-transposed grid of samples (to distinguish from multi-block filtering), the basic functions of block filtering
are simply named NT ZIC and NT ICC, where NT stands for non-transposed grid.

\begin{equation*}
    \label{eq:option_1}
        \begin{array}{c|c|c|c|c}
            \cline{2-2}
            \cline{4-4}
            \underrightarrow{\bm{x}} & \text{NT ZIC} & \underrightarrow{\bm{w}} & \text{NT ICC} & \underrightarrow{\bm{y}} \\
            \cline{2-2}
            \cline{4-4}
    \end{array}
\end{equation*}

2. The option 2: Compared with the option 1, the particular part of recursive equation is replaced by our proposed multi-block filtering,
which has the similar idea as in \cite{Jaewoo_09}.

\begin{equation*}
    \label{eq:option_2}
    \begin{aligned}
        & \begin{array}{c|c|c|c|c|c|c|c|c}
            \cline{2-2}
            \cline{4-4}
            \cline{6-6}
            \cline{8-8}
            \underrightarrow{\bm{X}} & \text{T} & \underrightarrow{\bm{X}^T} & \text{T ZIC} & \underrightarrow{\bm{W}^T} & \text{T} & \underrightarrow{\bm{W} (M \times \bm{w})} & \text{NT ICC} & \underrightarrow{\bm{y}} \\
            \cline{2-2}
            \cline{4-4}
            \cline{6-6}
            \cline{8-8}
    \end{array}
\end{aligned}
\end{equation*}

3. The option 3: Our proposed multi-block filtering algori\-thm, where both functions process transposed matrix of samples. 

\begin{equation*}
    \label{eq:option_3}
    \begin{aligned}
        & \begin{array}{c|c|c|c|c|c|c|c|c}
            \cline{2-2}
            \cline{4-4}
            \cline{6-6}
            \cline{8-8}
            \underrightarrow{\bm{X}} & \text{T} & \underrightarrow{\bm{X}^T} & \text{T ZIC} & \underrightarrow{\bm{W}^T} & \text{T ICC} & \underrightarrow{\bm{Y}^T} & \text{T} & \underrightarrow{\bm{Y}} \\
            \cline{2-2}
            \cline{4-4}
            \cline{6-6}
            \cline{8-8}
    \end{array}
\end{aligned}
\end{equation*}
% In the above block diagrams,
The notations of the data flow between functions in lowercase letters represent vector of samples,
while the uppercase letters mean matrix of samples. The T in box denotes the function of matrix transpose.
In option 2, $M \times \bm{w}$ means $M$ numbers of vector ($\bm{w}$) of samples.
Since the NT ICC only accepts vector of samples, the way it works is simply to passing the vectors in the matrix by order.


The number of FMAs, shuffles, broadcast and load instru\-ctions for functions 
and options are recorded in Table \ref{table:number_of_instructions}.
Due to the different sizes of samples processed in the different functions, 
the number of instructions are transferred to instructions per sample (IPS) for fair comparison.
The broadcast operation happens at when a FMA is taken between a scalar and vector, e.g.,
in \eqref{eq:block_filtering}, $\bm{A}_{[0]} \times \bm{y_p}[0]$, $\bm{y_p}[0]$ should be first broadcasted to
a vector in the same length of $\bm{A}_{[0]}$ including all elements of $\bm{y_p}[0]$, then multiply.
Furthermore, the number of store per sample for each function is identical as $1/M$, thus omitted. 

\begin{table}[t]
    \caption{The number of SIMD instructions (per sample) for functions and options in theory}  % title of Table
    \centering % used for centering table
    \setlength{\tabcolsep}{0.9pt}
    \begin{tabular}{c|c|c|c|c} % centered columns (4 columns)
    \hline\hline %inserts double horizontal lines
    function name & FMA & shuffle & broadcast & load \\ [0.5ex] % inserts table
    \hline % inserts single horizontal line
    NT ZIC & $1{+}\frac{2}{M}$ & 0 & $1{+}\frac{2}{M}$ & $1{+}\frac{5}{M}$ \\ [0.5ex]
    NT ICC & $\frac{2}{M}$ & 0 & $\frac{2}{M}$ & $\frac{5}{M}$ \\ [0.5ex]
    T ZIC & $\frac{4}{M}{-}\frac{3}{M^2}$ & $\frac{2}{M^2}$ & $\frac{4}{M^2}$ & $\frac{1}{M}{+}\frac{6}{M^2}$ \\ [0.5ex]
    T ICC & $\frac{4\log_2M}{M^2}{+}\frac{2}{M}$ & $\frac{2\log_2M{+}2}{M^2}$ & $\frac{2}{M}{-}\frac{2}{M^2}$ & $\frac{1}{M}{+}\frac{4\log_2M{+}8}{M^2}$ \\ [0.5ex]
    Transpose & 0 & $\frac{\log_2M}{M}$ & 0 & $\frac{1}{M}$ \\ [0.5ex]
    Option 1 & $1{+}\frac{4}{M}$ & 0 & $1{+}\frac{4}{M}$ & $1{+}\frac{9}{M}$ \\ [0.5ex]
    Option 2 & $\frac{6}{M}{-}\frac{3}{M^2}$ & $\frac{2\log_2M}{M}{+}\frac{2}{M^2}$ & $\frac{2}{M}{+}\frac{4}{M^2}$ & $\frac{3}{M}{+}\frac{8}{M^2}$ \\ [0.5ex]
    Option 3 & $\frac{6}{M}{+}\frac{4\log_2M{-}3}{M^2}$ & $\frac{2\log_2M}{M}{+}\frac{2\log_2M+4}{M^2}$ & $\frac{2}{M}{+}\frac{2}{M^2}$ & $\frac{1}{M}{+}\frac{4\log_2M{+}14}{M^2}$ \\ [1ex]
    \hline
    \end{tabular}
    \label{table:number_of_instructions} % is used to refer this table in the text
\end{table}

From Table \ref{table:number_of_instructions}, we see the
option 1 is relatively efficient when the processor is restricted to only deal with shorter length of SIMD instruction.
For modern CPUs, option 2 is the cheapest at the number of FMAs plus shuffles.
The feature of option 3 
is that two matrix transposes are located at the head and tail. 
If build a higher order recursive filter by cascading option 3,
the matrix transpose at the head of current section can be cancelled by the matrix transpose at the tail of previous section. 
Thus, the intermediate second order sections of option 3
only needs to take ZIC and ICC.
Then, the computing complexity of one section is reduce from $O(2\log_2M/M)$ to $O(6/M)$.

Based on the above discussion, we can see that our proposed algorithm is very suitable for building cascaded form of higher order recursive filter.
If the number of instructions is the only consideration, then for a $L$th order recursive filter,
the cheapest combination should be $L/2{-}1$ second order filters of option 3 followed by one option 2.
For example, the block diagram of composing a 10th order recursive filter can be given by

\begin{equation*}
    \label{eq:4_cascaded_option_3_1_option_2}
    \begin{aligned}
    & \arraycolsep=2.3pt\def\arraystretch{1.0}
    \begin{array}{c|c|c|c|c|c|c|c|c|c|}
        \cline{2-2}
        \cline{4-4}
        \cline{6-6}
        \cline{8-8}
        \cline{10-10}
        \underrightarrow{\bm{X}_{(1)}} & \text{T} & \underrightarrow{\bm{X}_{(1)}^T} & \text{T ZIC} & \underrightarrow{\bm{W}_{(1)}^T} & \text{T ICC} & \underrightarrow{\bm{Y}_{(1)}^T} & \text{T ZIC} & \underrightarrow{\bm{W}_{(2)}^T} & \text{T ICC} \\
        \cline{2-2}
        \cline{4-4}
        \cline{6-6}
        \cline{8-8}
        \cline{10-10}
    \end{array} \\
    & \arraycolsep=2.3pt\def\arraystretch{1.0} 
    \begin{array}{ccc|c|c|c|c|c|c}
        \cline{4-4}
        \cline{6-6}
        \cline{8-8}
        % \cline{10-10}
        \underrightarrow{\bm{Y}_{(2)}^T} & \cdots & \underrightarrow{\bm{Y}_{(4)}^T} & \text{T ZIC} & \underrightarrow{\bm{W}_{(5)}^T} & \text{T} & \underrightarrow{\bm{W}_{(5)}(M \times \bm{w}_{(5)})} & \text{NT ICC} & \underrightarrow{\bm{Y}_{(5)}} \\
        \cline{4-4}
        \cline{6-6}
        \cline{8-8}
        % \cline{10-10}
    \end{array} \\
\end{aligned}
\end{equation*}

However, there is one last issue should be considered at the (NT) ICC part of option 2,
i.e., compared with matrix filtering, the vector filtering suffers a strong dependency problem since 
the computation of current vector relies on the last two samples of previous output vector that is not reflected from the number of instructions.
Thus, the solution is, not only to measuring the efficiency of the functions or options 
by looking at the number of instructions, but going deeper to see the clock cycles, which 
directly points to the application's latency.

% measuring the efficiency of the options by just looking at the number of instructions,  
% we should go deeper to see the clock cycles of the functions and options, which directly points to the 
% application's latency.
% In block filtering algorithm, the computation from each vector
% relies on the last two samples of previous output vector. Thus, 
% NT ICC may suffer from a strong dependency problem, which degrades the efficiency,
% that can't be reflected from the number of instructions.
% The solution is, instead of just looking at the number of instructions,  
% we should go deeper to see the clock cycles of the functions and options, which directly points to the 
% application's latency.


























% Having introduced the solutions of a second order recursive filter, now we discuss and compare with different
% combinations of functions to build the second or higher order recursive filter from implementation perspective.
% Assume the input is a matrix of consecutive samples. Thus, for block filtering, the functions for solving particular and homogeneous
% parts should perform $M$ times.

% The option 1 only accepts non-transposed grid of samples, i.e., the block filtering method, which can be considered as the benchmark.
% The option 2 is that the particular part of recursive equation is solved by our multi-block filtering, 
% and the vector filtering works for homogeneous part, which 
% has the same idea as the multi-block filtering method in \cite{Jaewoo_09}.
% The option 3 is our proposed multi-block filtering algorithm,
% where both functions accept the transposed matrix of samples.
% From implementation perspective,
% we are not limited to count the number of FMAs and shuffles,
% but also the number of other SIMD instructions, 
% e.g., broadcast and load operations for functions and options.
% The results are recorded in Table \ref{table:number_of_instructions}.
% The number of store for each function is identical as $1/M$, thus omitted. 
 
% \begin{table}[t]
%     \caption{The number of SIMD instructions (per sample) for functions and options in theory       }  % title of Table
%     \centering % used for centering table
%     \setlength{\tabcolsep}{0.9pt}
%     \begin{tabular}{c|c|c|c|c} % centered columns (4 columns)
%     \hline\hline %inserts double horizontal lines
%     function name & FMA & shuffle & broadcast & load \\ [0.5ex] % inserts table
%     \hline % inserts single horizontal line
%     NT ZIC & $1{+}\frac{2}{M}$ & 0 & $1{+}\frac{2}{M}$ & $1{+}\frac{5}{M}$ \\ [0.5ex]
%     NT ICC & $\frac{2}{M}$ & 0 & $\frac{2}{M}$ & $\frac{5}{M}$ \\ [0.5ex]
%     T ZIC & $\frac{4}{M}{-}\frac{3}{M^2}$ & $\frac{2}{M^2}$ & $\frac{4}{M^2}$ & $\frac{1}{M}{+}\frac{6}{M^2}$ \\ [0.5ex]
%     T ICC & $\frac{4\log_2M}{M^2}{+}\frac{2}{M}$ & $\frac{2\log_2M{+}2}{M^2}$ & $\frac{2}{M}{-}\frac{2}{M^2}$ & $\frac{1}{M}{+}\frac{4\log_2M{+}8}{M^2}$ \\ [0.5ex]
%     Transpose & 0 & $\frac{\log_2M}{M}$ & 0 & $\frac{1}{M}$ \\ [0.5ex]
%     Option 1 & $1{+}\frac{4}{M}$ & 0 & $1{+}\frac{4}{M}$ & $1{+}\frac{9}{M}$ \\ [0.5ex]
%     Option 2 & $\frac{6}{M}{-}\frac{3}{M^2}$ & $\frac{2\log_2M}{M}{+}\frac{2}{M^2}$ & $\frac{2}{M}{+}\frac{4}{M^2}$ & $\frac{1}{M}{+}\frac{10}{M^2}$ \\ [0.5ex]
%     Option 3 & $\frac{6}{M}{+}\frac{4\log_2M{-}3}{M^2}$ & $\frac{2\log_2M}{M}{+}\frac{2\log_2M+4}{M^2}$ & $\frac{2}{M}{+}\frac{2}{M^2}$ & $\frac{1}{M}{+}\frac{4\log_2M{+}14}{M^2}$ \\ [1ex]
%     \hline
%     \end{tabular}
%     \label{table:number_of_instructions} % is used to refer this table in the text
% \end{table}

% From Table \ref{table:number_of_instructions}, we basically see that the algorithms are scaled linearly by the length of SIMD instructions,
% i.e., the number of instructions per sample is inversely propotional to $M$ or $M^2$. 
% NT ZIC (or NT ICC) simply denotes the function of block filtering method for solving the particular (or homogeneous) part of recursive equation,
% where NT stands for non-transposed grid of samples (the samples are processed consecutively in vector) to distinguish from transposed grid of samples
% in multi-block filtering. 
% Assume the algorithms are implemented on software.
% Note, the load and broadcast operations are normally cheap, moreover,
% are known to the compiler at the compile time. The smart compiler will interleave
% the two operations between FMAs and shuffles to reduce the clock cycles of the program.


% On the other hand, if we re-compare each function by the total number of FMAs and shuffles, we may get similar results as before since the complexity degree for basic functions don't change.

% Let's go beyond the function level to see the three options of compusing the second order filter. Option 1 is more useful when the processor is limited to deal with the SIMD instruction.
% For modern CPUs, option 2 is the cheapest at the number of FMAs and shuffles. The feature of option 3 is that the two matrix transposes
% are located at the head and tail (one for input to ZIC, one for getting correct order of samples). When a high order recursive filter is constructed by cascading second order sections,
% the matrix transpose at the head of current section 
% can be cancelled out by the matrix transpose at the tail of previous section. 
% Thus, each intermediate cascaded second order filters of option 3 only needs to take the functions of ZIC and ICC.
% Then, the computing complexity of the intermediate sections is reduce to $O(6/M)$.


% Then, the computing complexity of intermediate sections of option 3 is $O(\log_2M/M^2)$, which is smaller than the complexity $O(\log_2M/M)$
% of option 2.

% Now we can see that our proposed algorithm is very suitable for building cascaded form of high order recursive filter.
% If the number of instructions is only the consideration, then for a $L$th order recursive filter,
% the cheapest combination can be $L/2-1$ cascaded second order filters of option 3 (without intermediate matrix transpose)
% followed by 1 second order section of option 2.

% However, there is one last problem at ICC part of option 2. In block filtering algorithm, the computation from each vector
% relies on the last two samples of previous output vector. Thus, 
% NT ICC may suffer from a strong dependency problem, which degrades the efficiency,
% that can't be reflected from the number of instructions.
% The solution is, instead of just looking at the number of instructions,  
% we should go deeper to see the clock cycles of the functions and options.





% \subsection{Combination of a second (or higher) order IIR core}

% \subsubsection{The second order IIR core}

% \begin{table}[t]
%     \caption{The number of instructions for base functions in IIR filter}  % title of Table
%     \centering % used for centering table
%     \begin{tabular}{c c c c c c} % centered columns (4 columns)
%     \hline\hline %inserts double horizontal lines
%     function name & FMA & shuffle & broadcast & load & store \\ [1ex] % inserts table
%     \hline % inserts single horizontal line
%     Transposed FIR & $2M$ & 4 & 2 & $M{+}2$ & $M$ \\ [0.3ex]
%     Non-trans FIR & $2M$ & $2M$ & 2 & $M{+}2$ & $M{+}1$ \\ [0.3ex]
%     Transposed ZIC & $2M{-}3$ & 0 & 2 & $M{+}2$ & $M$ \\ [0.3ex]
%     Non-trans ZIC & $M^2$ & 0 & $M^2$ & $2M$ & $M$ \\ [0.3ex]
%     Transposed ICC & $6M$ & 4 & $4M{+}6$ & $5M{+}6$ & $M{+}4$ \\ [0.3ex]
%     Non-trans ICC & $2M$ & 0 & $2M$ & $M{+}2$ & $M$ \\ [0.3ex]
%     Matrix transpose & 0 & $M{\log_2}M$ & 0 & $M$ & $M$ \\ [1ex]
%     \hline
%     \end{tabular}
%     \label{table:number_of_instructions} % is used to refer this table in the text
% \end{table}

% In the previous section, lots of base functions of composing a second order IIR core by passing transposed or non-transposed data are introduced.
% We now count for the number of FMAs, shuffles, broadcast, load and store operations for each base function
% as shown in Table \ref{table:number_of_instructions}. Based on the number of instructions (FMAs and shuffles) for each function,
% we list three significant combinations of a second order IIR filter and compute the total number of instructions as follows: 

% 1. Core 1: all passed non-transposed data. 
% \begin{equation*}
%     \label{eq:iir_core_1_all_non}
%     % \bm{X} = \left[\begin{array}{c|c|c|c}
%     % x[0] & x[M] & \cdots & x[M^2{-}M] \\ 
%     % x[1] & x[M{+}1] & \cdots & x[M^2{-}M{+}1] \\
%     % \vdots & \vdots & \ddots & \vdots \
%     % x[M{-}1] & x[2M{-}1] &\cdots & x[M^2{-}1] \\
%     % \end{array}\right]
%         \begin{array}{c|c|c|c|c|c|c}
%             \cline{2-2}
%             \cline{4-4}
%             \cline{6-6}
%             \underrightarrow{\bm{X}} & \text{FIR} & \underrightarrow{\bm{V}} & \text{ZIC} & \underrightarrow{\bm{W}} & \text{ICC} & \underrightarrow{\bm{Y}} \\
%             \cline{2-2}
%             \cline{4-4}
%             \cline{6-6}
%     \end{array}
% \end{equation*}

% 2. Core 2: all passed transposed data.
% \begin{equation*}
%     \label{eq:iir_core_2_all_trans}
%     \begin{aligned}
%     % \bm{X} = \left[\begin{array}{c|c|c|c}
%     % x[0] & x[M] & \cdots & x[M^2{-}M] \\ 
%     % x[1] & x[M{+}1] & \cdots & x[M^2{-}M{+}1] \\
%     % \vdots & \vdots & \ddots & \vdots \\
%     % x[M{-}1] & x[2M{-}1] &\cdots & x[M^2{-}1] \\
%     % \end{array}\right]
%     & \underrightarrow{\bm{X}} \\
%         & \begin{array}{|c|c|c|c|c|c|c|c|c|c}
%             \cline{1-1}
%             \cline{3-3}
%             \cline{5-5}
%             \cline{7-7}
%             \cline{9-9}
%             \text{T} & \underrightarrow{\bm{X}^T} & \text{T FIR} & \underrightarrow{\bm{V}^T} & \text{T ZIC} & \underrightarrow{\bm{W}^T} & \text{T ICC} & \underrightarrow{\bm{Y}^T} & \text{T} & \underrightarrow{\bm{Y}} \\
%             \cline{1-1}
%             \cline{3-3}
%             \cline{5-5}
%             \cline{7-7}
%             \cline{9-9}
%     \end{array}
% \end{aligned}
% \end{equation*}

% 3. Core 3: only ICC passed non-transposed data.
% \begin{equation*}
%     \label{eq:iir_core_3_icc_non_trans}
%     \begin{aligned}
%     % \bm{X} = \left[\begin{array}{c|c|c|c}
%     % x[0] & x[M] & \cdots & x[M^2{-}M] \\ 
%     % x[1] & x[M{+}1] & \cdots & x[M^2{-}M{+}1] \\
%     % \vdots & \vdots & \ddots & \vdots \\
%     % x[M{-}1] & x[2M{-}1] &\cdots & x[M^2{-}1] \\
%     % \end{array}\right]
%     % & \underrightarrow{\bm{X}} \\
%         & \begin{array}{c|c|c|c|c|c|c|c|c|c|c}
%             \cline{2-2}
%             \cline{4-4}
%             \cline{6-6}
%             \cline{8-8}
%             \cline{10-10}
%             \underrightarrow{\bm{X}} & \text{T} & \underrightarrow{\bm{X}^T} & \text{T FIR} & \underrightarrow{\bm{V}^T} & \text{T ZIC} & \underrightarrow{\bm{W}^T} & \text{T} & \underrightarrow{\bm{W}} & \text{ICC} & \underrightarrow{\bm{Y}} \\
%             \cline{2-2}
%             \cline{4-4}
%             \cline{6-6}
%             \cline{8-8}
%             \cline{10-10}
%     \end{array}
% \end{aligned}
% \end{equation*}
% The total cost for three cores are: $M^2{+}6M$, $2M{\log_2}M{+}10M{+}5$, $2M{\log_2}M{+}6M{+}1$, respectively.
% Normally, the length of SIMD vector is short, e.g., 4 or 8. Let's assume $M=8$ to achieve the better
% parallelism. Thus, the cost for three cores are calculated as 112, 133 and 97.

% If we consider the basic situation, i.e., a single CPU and single thread are used, core 3 should be
% the most instruction-saving. The feature of core 2 is that the two matrix transpose functions exist at the tail and head.
% If we construct a higher order IIR filter by cascading multiple second order IIR cores, the two matrix transpose functions
% at the tail and head can be cancelled out. In this case, the cost for core 2 can be then reduced to $10M{+}5=85$, which is cheaper
% than core 3. The advantage of core 1 is straightforward thus avoids two matrix transpose functions.
% Furthermore, if we consider a multi-CPU and multi-thread environment, the most expensive function of core 1, i.e., non-transposed ZIC,
% can be executed by inter-block parallelism and the real number of instructions can be reduce from $M^2$ to $M$. In this case, core 1 will be the most instruction-saving.

% \subsubsection{The higher order IIR system}

% Let's discuss the higher order IIR system by cascading the proposed three second order IIR cores.
% Take fourth order IIR system as an example.
% Again, if multiple CPUs and threads are used, the cascade of core 1 will be the most efficient. 
% If not, the most instruction-saving combination is core 2 followed by core 3, i.e.,

% \begin{equation*}
%     \label{eq:iir_system_4_core2_core3}
%     \begin{aligned}
%     % \bm{X} = \left[\begin{array}{c|c|c|c}
%     % x[0] & x[M] & \cdots & x[M^2{-}M] \\ 
%     % x[1] & x[M{+}1] & \cdots & x[M^2{-}M{+}1] \\
%     % \vdots & \vdots & \ddots & \vdots \\
%     % x[M{-}1] & x[2M{-}1] &\cdots & x[M^2{-}1] \\
%     % \end{array}\right]
%     & \underrightarrow{\bm{X}_{(1)}} \\
%     & \begin{array}{|c|c|c|c|c|c|c|c}
%         \cline{1-1}
%         \cline{3-3}
%         \cline{5-5}
%         \cline{7-7}
%         % \cline{10-10}
%         \text{T} & \underrightarrow{\bm{X}_{(1)}^T} & \text{T FIR} & \underrightarrow{\bm{V}_{(1)}^T} & \text{T ZIC} & \underrightarrow{\bm{W}_{(1)}^T} & \text{T ICC} & \underrightarrow{\bm{Y}_{(1)}^T(\bm{X}_{(2)}^T)} \\
%         \cline{1-1}
%         \cline{3-3}
%         \cline{5-5}
%         \cline{7-7}
%         % \cline{10-10}
%     \end{array} \\
%         & \begin{array}{c|c|c|c|c|c|c|c|c}
%             \cline{2-2}
%             \cline{4-4}
%             \cline{6-6}
%             \cline{8-8}
%             \underrightarrow{} & \text{T FIR} & \underrightarrow{\bm{V}_{(2)}^T} & \text{T ZIC} & \underrightarrow{\bm{W}_{(2)}^T} & \text{T} & \underrightarrow{\bm{W}_{(2)}} & \text{T ICC}  & \underrightarrow{\bm{Y}_{(2)}} \\
%             \cline{2-2}
%             \cline{4-4}
%             \cline{6-6}
%             \cline{8-8}
%     \end{array}
% \end{aligned}
% \end{equation*}
% The cost for the above fourth order IIR is $2M{\log_2}M{+}16M{+}6 = 182$. Thus, we can see that, for a $L(\geq 4)$th order IIR filter, if only a single CPU and thread is used,
% the optimal combination should be $L/2{-}1$ cascaded second order core 2 followed by one core 3 at the end.

% \subsection{Base function optimization}

% \begin{table}[t]
%     \caption{The number of cycles for each SIMD instruction in VCL}  % title of Table
%     \centering % used for centering table
%     \begin{tabular}{c c c c c c} % centered columns (4 columns)
%     \hline\hline %inserts double horizontal lines
%      & FMA & shuffle & broadcast & load & store \\ [1ex] % inserts table
%     \hline % inserts single horizontal line
%     Number of cycles & 4 & 2-3 & 2-3 & 0-1 & 0-1 \\ [0.3ex]
%     Throughput & 0.5 & 1 & 0.5 & 0.25 & 0.25 \\ [1ex]
%     \hline
%     \end{tabular}
%     \label{table:number_of_cycles} % is used to refer this table in the text
% \end{table}

% We should note that the problem that we discussed so far has two assumptions: 1. The number of cycles for broadcast,
% load and store operations are ignored. This will be true if the three operations take less cycles than FMA and shuffle since
% those operations will be inserted between FMAs and executed within the duration of FMAs. 
% 2. One FMA or shuffle instruction only takes one cycle. However, this is not always true.
% The cost of every SIMD instruction depends on what platform and micro-architecture are used to be implemented. For example, 
% in this paper, we implement the SIMD instructions on software via C++ programming by the mean of vector class library (VCL).
% The computer has the Intel Skylake CPU. The number of cycles that each instruction actually takes is listed in Table \ref{table:number_of_cycles},
% where the (reciprocal) throughput means the maximum number of instructions can be executed per clock cycle, e.g., the 0.5 throughput of FMA means
% at most 2 FMAs can be executed in one clock cycle.

% Because the FMA and shuffle instructions need to take more than one cycle in VCL, different computation strategy will affect the efficiency of base functions.
% We propose below three basic methods of improving the efficiency of base functions:

% 1. Interleave dependency. When the function of transposed ZIC in \eqref{eq:ZIC_w_trans} is executed, the inherent dependency should be paid attention to.
% By unrolling the loop and exchanging the order of FMAs for SIMD vector, \eqref{eq:ZIC_w_trans} can be computed more efficiently as

% \begin{equation}
%     \label{eq:interleave_dependency}
%     \begin{aligned}
%         & \bm{W}^T[0] = \bm{V}^T[0]; \\
%         & \bm{W}^T[1] = \text{mul\_add}(\bm{V}^T[0],a_1,\bm{V}^T[1]); \\
%         & \bm{W}^T[2] = \text{mul\_add}(\bm{W}^T[0],a_2,\bm{V}^T[2]); \\
%         & \bm{W}^T[3] = \text{mul\_add}(\bm{W}^T[1],a_2,\bm{V}^T[3]); \\
%         & \bm{W}^T[2] = \text{mul\_add}(\bm{W}^T[1],a_1,\bm{W}^T[2]); \\
%         & \bm{W}^T[3] = \text{mul\_add}(\bm{W}^T[2],a_1,\bm{W}^T[3]); \\
%         & \bm{W}^T[4] = \text{mul\_add}(\bm{W}^T[2],a_2,\bm{V}^T[4]); \\
%         & \quad\quad\quad\quad\quad\quad\quad\quad\quad \vdots
%     \end{aligned}
% \end{equation}
% where mul\_add($\cdot$) is the FMA instruction for SIMD operation in VCL.
% We can see that by exchanging the order of FMAs, each FMA is interleaved by two cycles, i.e.,
% the efficiency is increased roughly by two times compared with computing in order. 

% 2. Parallelize large sum-up. The expensive part of computing transposed ICC is the big matrix multiplication in \eqref{eq:ICC_init_w_trans}.
% The computation is essentially summing up all the FMAs without dependency for each of two blocks. Thus, we can compute and sum up the FMAs in parallel, and then
% add the sub sum-up together. For example, when calculate \eqref{eq:ICC_init_w_trans} in parallel two,

% \begin{equation}
%     \label{eq:parallelized_sum_up}
%     \begin{aligned}
%         & \bm{yt} = \text{mul\_add}(\bm{T}[0],\bm{W}^T[M{-}2][0],\bm{yt}); \\
%         & \bm{ys} = \text{mul\_add}(\bm{T}[M],\bm{W}^T[M{-}1][0],\bm{ys}); \\
%         & \bm{yt} = \text{mul\_add}(\bm{T}[1],\bm{W}^T[M{-}2][1],\bm{yt}); \\
%         & \bm{ys} = \text{mul\_add}(\bm{T}[M{+}1],\bm{W}^T[M{-}1][1],\bm{ys}); \\
%         & \bm{yl} = \text{mul\_add}(\bm{T}[2M],\bm{W}^T[M{-}2][0],\bm{yl}); \\
%         & \bm{yk} = \text{mul\_add}(\bm{T}[3M],\bm{W}^T[M{-}1][0],\bm{yk}); \\
%         & \bm{yl} = \text{mul\_add}(\bm{T}[2M{+}1],\bm{W}^T[M{-}2][1],\bm{yl}); \\
%         & \bm{yk} = \text{mul\_add}(\bm{T}[3M{+}1],\bm{W}^T[M{-}1][1],\bm{yk}); \\
%         & \quad\quad\quad\quad\quad\quad\quad\quad\quad \vdots
%     \end{aligned}
% \end{equation}
% where $\bm{yt}$, $\bm{ys}$ are two SIMD temporary registers for calculating $\bm{Y}^T[M{-}2]$, and
% $\bm{yl}$, $\bm{yk}$ are registers for calculating $\bm{Y}^T[M{-}1]$. It can be seen that by computing the 
% large sum-up in parallel, we increase the number of FMAs computed simultaneously to approach the throughput of FMA.
% Note, we can also exchange the order of computing $\bm{yt}$, $\bm{ys}$ and $\bm{yl}$, $\bm{yk}$ 
% to improve the efficiency as interleaving.

% 3. Mix FIR and ZIC. When construct a high order IIR filter, as we discussed, core 2 based on transposed data will be used 
% for multiple times. Compared with transposed ICC, transposed FIR and ZIC functions are much cheaper, thus may induce a large overhead.
% On the other hand, to mix executing FIR and ZIC, the inherent dependency of ZIC can be further interleaved by the operations in FIR,

% \begin{equation}
%     \label{eq:mix_fir_zic}
%     \begin{aligned}
%         & \bm{xp2} = \text{blend}(\bm{X}^T[M{-}2],x_{-2}); \\
%         & \bm{xp1} = \text{blend}(\bm{X}^T[M{-}1],x_{-1}); \\
%         & \bm{V}^T[0] = \text{mul\_add}(\bm{xp2},b_2,\bm{X}^T[0]); \\
%         & \bm{V}^T[0] = \text{mul\_add}(\bm{xp1},b_1,\bm{V}^T[0]); \\
%         & \bm{W}^T[0] = \bm{V}^T[0]; \\
%         & \bm{V}^T[1] = \text{mul\_add}(\bm{xp1},b_2,\bm{X}^T[1]); \\
%         & \bm{V}^T[1] = \text{mul\_add}(\bm{X}^T[0],b_1,\bm{V}^T[1]); \\
%         & \bm{W}^T[1] = \text{mul\_add}(\bm{V}^T[0],a_1,\bm{V}^T[1]); \\
%         & \quad\quad\quad\quad\quad\quad\quad\quad\quad \vdots
%     \end{aligned}
% \end{equation}
% where blend is the shuffle instruction in VCL. 
